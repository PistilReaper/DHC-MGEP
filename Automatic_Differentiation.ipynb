{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "440d0be2-52f0-47a5-aaac-11aa32c23475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x24a4e5ad510>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tkinter.messagebox import NO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "import time\n",
    "import scipy.io as scio\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57682764-e78f-49b4-992b-d58a8d896724",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4ea35f4-65c9-4eff-929a-12bb5a18a02d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Adam Loss: tensor(4.8491, device='cuda:0')\n",
      "200 Adam Loss: tensor(1.8741, device='cuda:0')\n",
      "300 Adam Loss: tensor(1.0215, device='cuda:0')\n",
      "400 Adam Loss: tensor(0.6425, device='cuda:0')\n",
      "500 Adam Loss: tensor(0.3702, device='cuda:0')\n",
      "600 Adam Loss: tensor(0.2117, device='cuda:0')\n",
      "700 Adam Loss: tensor(0.1280, device='cuda:0')\n",
      "800 Adam Loss: tensor(0.0879, device='cuda:0')\n",
      "900 Adam Loss: tensor(0.0644, device='cuda:0')\n",
      "1000 Adam Loss: tensor(0.0477, device='cuda:0')\n",
      "1100 Adam Loss: tensor(0.0376, device='cuda:0')\n",
      "1200 Adam Loss: tensor(0.0305, device='cuda:0')\n",
      "1300 Adam Loss: tensor(0.0258, device='cuda:0')\n",
      "1400 Adam Loss: tensor(0.0217, device='cuda:0')\n",
      "1500 Adam Loss: tensor(0.0183, device='cuda:0')\n",
      "1600 Adam Loss: tensor(0.0159, device='cuda:0')\n",
      "1700 Adam Loss: tensor(0.0140, device='cuda:0')\n",
      "1800 Adam Loss: tensor(0.0124, device='cuda:0')\n",
      "1900 Adam Loss: tensor(0.0121, device='cuda:0')\n",
      "2000 Adam Loss: tensor(0.0101, device='cuda:0')\n",
      "2100 Adam Loss: tensor(0.0095, device='cuda:0')\n",
      "2200 Adam Loss: tensor(0.0092, device='cuda:0')\n",
      "2300 Adam Loss: tensor(0.0089, device='cuda:0')\n",
      "2400 Adam Loss: tensor(0.0093, device='cuda:0')\n",
      "2500 Adam Loss: tensor(0.0069, device='cuda:0')\n",
      "2600 Adam Loss: tensor(0.0085, device='cuda:0')\n",
      "2700 Adam Loss: tensor(0.0064, device='cuda:0')\n",
      "2800 Adam Loss: tensor(0.0068, device='cuda:0')\n",
      "2900 Adam Loss: tensor(0.0063, device='cuda:0')\n",
      "3000 Adam Loss: tensor(0.0055, device='cuda:0')\n",
      "3100 Adam Loss: tensor(0.0047, device='cuda:0')\n",
      "3200 Adam Loss: tensor(0.0045, device='cuda:0')\n",
      "3300 Adam Loss: tensor(0.0050, device='cuda:0')\n",
      "3400 Adam Loss: tensor(0.0046, device='cuda:0')\n",
      "3500 Adam Loss: tensor(0.0047, device='cuda:0')\n",
      "3600 Adam Loss: tensor(0.0042, device='cuda:0')\n",
      "3700 Adam Loss: tensor(0.0084, device='cuda:0')\n",
      "3800 Adam Loss: tensor(0.0036, device='cuda:0')\n",
      "3900 Adam Loss: tensor(0.0048, device='cuda:0')\n",
      "4000 Adam Loss: tensor(0.0045, device='cuda:0')\n",
      "4100 Adam Loss: tensor(0.0068, device='cuda:0')\n",
      "4200 Adam Loss: tensor(0.0033, device='cuda:0')\n",
      "4300 Adam Loss: tensor(0.0032, device='cuda:0')\n",
      "4400 Adam Loss: tensor(0.0031, device='cuda:0')\n",
      "4500 Adam Loss: tensor(0.0030, device='cuda:0')\n",
      "4600 Adam Loss: tensor(0.0030, device='cuda:0')\n",
      "4700 Adam Loss: tensor(0.0091, device='cuda:0')\n",
      "4800 Adam Loss: tensor(0.0037, device='cuda:0')\n",
      "4900 Adam Loss: tensor(0.0111, device='cuda:0')\n",
      "5000 Adam Loss: tensor(0.0038, device='cuda:0')\n",
      "5100 Adam Loss: tensor(0.0026, device='cuda:0')\n",
      "5200 Adam Loss: tensor(0.0028, device='cuda:0')\n",
      "5300 Adam Loss: tensor(0.0031, device='cuda:0')\n",
      "5400 Adam Loss: tensor(0.0026, device='cuda:0')\n",
      "5500 Adam Loss: tensor(0.0024, device='cuda:0')\n",
      "5600 Adam Loss: tensor(0.0024, device='cuda:0')\n",
      "5700 Adam Loss: tensor(0.0024, device='cuda:0')\n",
      "5800 Adam Loss: tensor(0.0024, device='cuda:0')\n",
      "5900 Adam Loss: tensor(0.0023, device='cuda:0')\n",
      "6000 Adam Loss: tensor(0.0024, device='cuda:0')\n",
      "6100 Adam Loss: tensor(0.0059, device='cuda:0')\n",
      "6200 Adam Loss: tensor(0.0038, device='cuda:0')\n",
      "6300 Adam Loss: tensor(0.0033, device='cuda:0')\n",
      "6400 Adam Loss: tensor(0.0021, device='cuda:0')\n",
      "6500 Adam Loss: tensor(0.0029, device='cuda:0')\n",
      "6600 Adam Loss: tensor(0.0029, device='cuda:0')\n",
      "6700 Adam Loss: tensor(0.0022, device='cuda:0')\n",
      "6800 Adam Loss: tensor(0.0021, device='cuda:0')\n",
      "6900 Adam Loss: tensor(0.0039, device='cuda:0')\n",
      "7000 Adam Loss: tensor(0.0039, device='cuda:0')\n",
      "7100 Adam Loss: tensor(0.0025, device='cuda:0')\n",
      "7200 Adam Loss: tensor(0.0021, device='cuda:0')\n",
      "7300 Adam Loss: tensor(0.0031, device='cuda:0')\n",
      "7400 Adam Loss: tensor(0.0025, device='cuda:0')\n",
      "7500 Adam Loss: tensor(0.0019, device='cuda:0')\n",
      "7600 Adam Loss: tensor(0.0020, device='cuda:0')\n",
      "7700 Adam Loss: tensor(0.0049, device='cuda:0')\n",
      "7800 Adam Loss: tensor(0.0054, device='cuda:0')\n",
      "7900 Adam Loss: tensor(0.0033, device='cuda:0')\n",
      "8000 Adam Loss: tensor(0.0018, device='cuda:0')\n",
      "8100 Adam Loss: tensor(0.0019, device='cuda:0')\n",
      "8200 Adam Loss: tensor(0.0020, device='cuda:0')\n",
      "8300 Adam Loss: tensor(0.0018, device='cuda:0')\n",
      "8400 Adam Loss: tensor(0.0018, device='cuda:0')\n",
      "8500 Adam Loss: tensor(0.0018, device='cuda:0')\n",
      "8600 Adam Loss: tensor(0.0022, device='cuda:0')\n",
      "8700 Adam Loss: tensor(0.0057, device='cuda:0')\n",
      "8800 Adam Loss: tensor(0.0043, device='cuda:0')\n",
      "8900 Adam Loss: tensor(0.0026, device='cuda:0')\n",
      "9000 Adam Loss: tensor(0.0018, device='cuda:0')\n",
      "9100 Adam Loss: tensor(0.0017, device='cuda:0')\n",
      "9200 Adam Loss: tensor(0.0054, device='cuda:0')\n",
      "9300 Adam Loss: tensor(0.0020, device='cuda:0')\n",
      "9400 Adam Loss: tensor(0.0037, device='cuda:0')\n",
      "9500 Adam Loss: tensor(0.0018, device='cuda:0')\n",
      "9600 Adam Loss: tensor(0.0021, device='cuda:0')\n",
      "9700 Adam Loss: tensor(0.0028, device='cuda:0')\n",
      "9800 Adam Loss: tensor(0.0045, device='cuda:0')\n",
      "9900 Adam Loss: tensor(0.0033, device='cuda:0')\n",
      "10000 Adam Loss: tensor(0.0016, device='cuda:0')\n",
      "10100 Adam Loss: tensor(0.0016, device='cuda:0')\n",
      "10200 Adam Loss: tensor(0.0016, device='cuda:0')\n",
      "10300 Adam Loss: tensor(0.0019, device='cuda:0')\n",
      "10400 Adam Loss: tensor(0.0017, device='cuda:0')\n",
      "10500 Adam Loss: tensor(0.0023, device='cuda:0')\n",
      "10600 Adam Loss: tensor(0.0016, device='cuda:0')\n",
      "10700 Adam Loss: tensor(0.0016, device='cuda:0')\n",
      "10800 Adam Loss: tensor(0.0016, device='cuda:0')\n",
      "10900 Adam Loss: tensor(0.0018, device='cuda:0')\n",
      "11000 Adam Loss: tensor(0.0120, device='cuda:0')\n",
      "11100 Adam Loss: tensor(0.0139, device='cuda:0')\n",
      "11200 Adam Loss: tensor(0.0016, device='cuda:0')\n",
      "11300 Adam Loss: tensor(0.0016, device='cuda:0')\n",
      "11400 Adam Loss: tensor(0.0016, device='cuda:0')\n",
      "11500 Adam Loss: tensor(0.0016, device='cuda:0')\n",
      "11600 Adam Loss: tensor(0.0017, device='cuda:0')\n",
      "11700 Adam Loss: tensor(0.0047, device='cuda:0')\n",
      "11800 Adam Loss: tensor(0.0038, device='cuda:0')\n",
      "11900 Adam Loss: tensor(0.0022, device='cuda:0')\n",
      "12000 Adam Loss: tensor(0.0019, device='cuda:0')\n",
      "12100 Adam Loss: tensor(0.0017, device='cuda:0')\n",
      "12200 Adam Loss: tensor(0.0016, device='cuda:0')\n",
      "12300 Adam Loss: tensor(0.0016, device='cuda:0')\n",
      "12400 Adam Loss: tensor(0.0019, device='cuda:0')\n",
      "12500 Adam Loss: tensor(0.0015, device='cuda:0')\n",
      "12600 Adam Loss: tensor(0.0044, device='cuda:0')\n",
      "12700 Adam Loss: tensor(0.0033, device='cuda:0')\n",
      "12800 Adam Loss: tensor(0.0015, device='cuda:0')\n",
      "12900 Adam Loss: tensor(0.0015, device='cuda:0')\n",
      "13000 Adam Loss: tensor(0.0015, device='cuda:0')\n",
      "13100 Adam Loss: tensor(0.0023, device='cuda:0')\n",
      "13200 Adam Loss: tensor(0.0015, device='cuda:0')\n",
      "13300 Adam Loss: tensor(0.0015, device='cuda:0')\n",
      "13400 Adam Loss: tensor(0.0016, device='cuda:0')\n",
      "13500 Adam Loss: tensor(0.0015, device='cuda:0')\n",
      "13600 Adam Loss: tensor(0.0015, device='cuda:0')\n",
      "13700 Adam Loss: tensor(0.0016, device='cuda:0')\n",
      "13800 Adam Loss: tensor(0.0051, device='cuda:0')\n",
      "13900 Adam Loss: tensor(0.0015, device='cuda:0')\n",
      "14000 Adam Loss: tensor(0.0015, device='cuda:0')\n",
      "14100 Adam Loss: tensor(0.0029, device='cuda:0')\n",
      "14200 Adam Loss: tensor(0.0015, device='cuda:0')\n",
      "14300 Adam Loss: tensor(0.0015, device='cuda:0')\n",
      "14400 Adam Loss: tensor(0.0029, device='cuda:0')\n",
      "14500 Adam Loss: tensor(0.0035, device='cuda:0')\n",
      "14600 Adam Loss: tensor(0.0041, device='cuda:0')\n",
      "14700 Adam Loss: tensor(0.0014, device='cuda:0')\n",
      "14800 Adam Loss: tensor(0.0015, device='cuda:0')\n",
      "14900 Adam Loss: tensor(0.0015, device='cuda:0')\n",
      "15000 Adam Loss: tensor(0.0022, device='cuda:0')\n",
      "15100 Adam Loss: tensor(0.0016, device='cuda:0')\n",
      "15200 Adam Loss: tensor(0.0014, device='cuda:0')\n",
      "15300 Adam Loss: tensor(0.0044, device='cuda:0')\n",
      "15400 Adam Loss: tensor(0.0027, device='cuda:0')\n",
      "15500 Adam Loss: tensor(0.0015, device='cuda:0')\n",
      "15600 Adam Loss: tensor(0.0014, device='cuda:0')\n",
      "15700 Adam Loss: tensor(0.0014, device='cuda:0')\n",
      "15800 Adam Loss: tensor(0.0018, device='cuda:0')\n",
      "15900 Adam Loss: tensor(0.0104, device='cuda:0')\n",
      "16000 Adam Loss: tensor(0.0029, device='cuda:0')\n",
      "16100 Adam Loss: tensor(0.0020, device='cuda:0')\n",
      "16200 Adam Loss: tensor(0.0014, device='cuda:0')\n",
      "16300 Adam Loss: tensor(0.0014, device='cuda:0')\n",
      "16400 Adam Loss: tensor(0.0014, device='cuda:0')\n",
      "16500 Adam Loss: tensor(0.0015, device='cuda:0')\n",
      "16600 Adam Loss: tensor(0.0014, device='cuda:0')\n",
      "16700 Adam Loss: tensor(0.0173, device='cuda:0')\n",
      "16800 Adam Loss: tensor(0.0014, device='cuda:0')\n",
      "16900 Adam Loss: tensor(0.0014, device='cuda:0')\n",
      "17000 Adam Loss: tensor(0.0020, device='cuda:0')\n",
      "17100 Adam Loss: tensor(0.0014, device='cuda:0')\n",
      "17200 Adam Loss: tensor(0.0015, device='cuda:0')\n",
      "17300 Adam Loss: tensor(0.0064, device='cuda:0')\n",
      "17400 Adam Loss: tensor(0.0015, device='cuda:0')\n",
      "17500 Adam Loss: tensor(0.0014, device='cuda:0')\n",
      "17600 Adam Loss: tensor(0.0016, device='cuda:0')\n",
      "17700 Adam Loss: tensor(0.0014, device='cuda:0')\n",
      "17800 Adam Loss: tensor(0.0031, device='cuda:0')\n",
      "17900 Adam Loss: tensor(0.0014, device='cuda:0')\n",
      "18000 Adam Loss: tensor(0.0014, device='cuda:0')\n",
      "18100 Adam Loss: tensor(0.0017, device='cuda:0')\n",
      "18200 Adam Loss: tensor(0.0061, device='cuda:0')\n",
      "18300 Adam Loss: tensor(0.0016, device='cuda:0')\n",
      "18400 Adam Loss: tensor(0.0014, device='cuda:0')\n",
      "18500 Adam Loss: tensor(0.0015, device='cuda:0')\n",
      "18600 Adam Loss: tensor(0.0022, device='cuda:0')\n",
      "18700 Adam Loss: tensor(0.0015, device='cuda:0')\n",
      "18800 Adam Loss: tensor(0.0020, device='cuda:0')\n",
      "18900 Adam Loss: tensor(0.0015, device='cuda:0')\n",
      "19000 Adam Loss: tensor(0.0014, device='cuda:0')\n",
      "19100 Adam Loss: tensor(0.0014, device='cuda:0')\n",
      "19200 Adam Loss: tensor(0.0014, device='cuda:0')\n",
      "19300 Adam Loss: tensor(0.0033, device='cuda:0')\n",
      "19400 Adam Loss: tensor(0.0014, device='cuda:0')\n",
      "19500 Adam Loss: tensor(0.0014, device='cuda:0')\n",
      "19600 Adam Loss: tensor(0.0017, device='cuda:0')\n",
      "19700 Adam Loss: tensor(0.0016, device='cuda:0')\n",
      "19800 Adam Loss: tensor(0.0013, device='cuda:0')\n",
      "19900 Adam Loss: tensor(0.0090, device='cuda:0')\n",
      "20000 Adam Loss: tensor(0.0013, device='cuda:0')\n",
      "0 L-BFGS-B Loss: 0.0013385715428739786\n",
      "1 L-BFGS-B Loss: 0.0008954895893111825\n",
      "2 L-BFGS-B Loss: 0.0008935104706324637\n",
      "3 L-BFGS-B Loss: 0.0008912186603993177\n",
      "4 L-BFGS-B Loss: 0.0008912183693610132\n",
      "5 L-BFGS-B Loss: 0.0008876322535797954\n",
      "6 L-BFGS-B Loss: 0.000887463625986129\n",
      "7 L-BFGS-B Loss: 0.00088740442879498\n",
      "8 L-BFGS-B Loss: 0.0008868324221111834\n",
      "9 L-BFGS-B Loss: 0.0008852323517203331\n",
      "10 L-BFGS-B Loss: 0.0008841398521326482\n",
      "11 L-BFGS-B Loss: 0.0008808508282527328\n",
      "12 L-BFGS-B Loss: 0.0008786342805251479\n",
      "13 L-BFGS-B Loss: 0.000878361111972481\n",
      "14 L-BFGS-B Loss: 0.000878071878105402\n",
      "15 L-BFGS-B Loss: 0.0008776318281888962\n",
      "16 L-BFGS-B Loss: 0.0008769622072577477\n",
      "17 L-BFGS-B Loss: 0.0008765426464378834\n",
      "18 L-BFGS-B Loss: 0.0008762466604821384\n",
      "19 L-BFGS-B Loss: 0.0008761900826357305\n",
      "20 L-BFGS-B Loss: 0.0008761414210312068\n",
      "21 L-BFGS-B Loss: 0.000875223777256906\n",
      "22 L-BFGS-B Loss: 0.0008751996210776269\n",
      "23 L-BFGS-B Loss: 0.0008751387940719724\n",
      "24 L-BFGS-B Loss: 0.0008750188280828297\n",
      "25 L-BFGS-B Loss: 0.0008746098028495908\n",
      "26 L-BFGS-B Loss: 0.0008745524100959301\n",
      "27 L-BFGS-B Loss: 0.0008723452920094132\n",
      "28 L-BFGS-B Loss: 0.0008723267819732428\n",
      "29 L-BFGS-B Loss: 0.0008718997705727816\n",
      "30 L-BFGS-B Loss: 0.000871875905431807\n",
      "31 L-BFGS-B Loss: 0.0008718108292669058\n",
      "32 L-BFGS-B Loss: 0.0008717924356460571\n",
      "33 L-BFGS-B Loss: 0.0008717641467228532\n",
      "34 L-BFGS-B Loss: 0.0008716805023141205\n",
      "35 L-BFGS-B Loss: 0.0008710129768587649\n",
      "36 L-BFGS-B Loss: 0.0008706165244802833\n",
      "37 L-BFGS-B Loss: 0.0008702364866621792\n",
      "38 L-BFGS-B Loss: 0.0008702364866621792\n",
      "Optimization finished.\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    # Initialize the class\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_layer1 = nn.Linear(2,20)\n",
    "        self.hidden_layer2 = nn.Linear(20,20)\n",
    "        self.hidden_layer3 = nn.Linear(20,20)\n",
    "        self.hidden_layer4 = nn.Linear(20,20)\n",
    "        self.hidden_layer5 = nn.Linear(20,20)\n",
    "        self.hidden_layer6 = nn.Linear(20,20)\n",
    "        self.hidden_layer7 = nn.Linear(20,20)\n",
    "        self.hidden_layer8 = nn.Linear(20,20)\n",
    "        self.hidden_layer9 = nn.Linear(20,20)\n",
    "        self.output_layer = nn.Linear(20,2)\n",
    "\n",
    "        for layer in [self.hidden_layer1, self.hidden_layer2, self.hidden_layer3, \n",
    "                      self.hidden_layer4, self.hidden_layer5, self.hidden_layer6, \n",
    "                      self.hidden_layer7, self.hidden_layer8, self.hidden_layer9, \n",
    "                      self.output_layer]:\n",
    "            init.xavier_uniform_(layer.weight)\n",
    "            init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        inputs = torch.cat([x, y], axis = 1)\n",
    "        layer1_out = torch.tanh(self.hidden_layer1(inputs))\n",
    "        layer2_out = torch.tanh(self.hidden_layer2(layer1_out))\n",
    "        layer3_out = torch.tanh(self.hidden_layer3(layer2_out))\n",
    "        layer4_out = torch.tanh(self.hidden_layer4(layer3_out))\n",
    "        layer5_out = torch.tanh(self.hidden_layer5(layer4_out))\n",
    "        layer6_out = torch.tanh(self.hidden_layer6(layer5_out))\n",
    "        layer7_out = torch.tanh(self.hidden_layer7(layer6_out))\n",
    "        layer8_out = torch.tanh(self.hidden_layer8(layer7_out))\n",
    "        layer9_out = torch.tanh(self.hidden_layer9(layer8_out))\n",
    "        output = self.output_layer(layer9_out)\n",
    "        return output\n",
    "        \n",
    "    def net_NS(self, x, y):\n",
    "        w = self.forward(x, y)\n",
    "        \n",
    "        u = w[:,0:1]\n",
    "        v = w[:,1:2]\n",
    "        \n",
    "        return [u, v]\n",
    "    \n",
    "    def net_diff(self, x, y):\n",
    "        w = self.forward(x, y)\n",
    "        \n",
    "        u = w[:,0:1]\n",
    "        v = w[:,1:2]\n",
    "        \n",
    "        u_x = torch.autograd.grad(u.sum(), x, create_graph = True)[0] \n",
    "        u_y = torch.autograd.grad(u.sum(), y, create_graph = True)[0]\n",
    "        v_x = torch.autograd.grad(v.sum(), x, create_graph = True)[0]\n",
    "        v_y = torch.autograd.grad(v.sum(), y, create_graph = True)[0]\n",
    "\n",
    "        return [u_x, u_y, v_x, v_y] \n",
    "\n",
    "    def predict(self, x, y):\n",
    "        return self.net_diff(x, y), self.net_NS(x,y)\n",
    "        \n",
    "\n",
    "def closure():\n",
    "    if torch.is_grad_enabled():\n",
    "        adam_optimizer.zero_grad()\n",
    "    outputs = net(x_device, y_device)\n",
    "    l = loss_fn(outputs, targets)\n",
    "    if l.requires_grad:\n",
    "        l.backward()\n",
    "    return l\n",
    "\n",
    "net = Net()\n",
    "net = Net().to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "adam_optimizer = torch.optim.Adam(net.parameters())\n",
    "lbfgs_optimizer = torch.optim.LBFGS(net.parameters(), lr=1, max_iter=50000, max_eval=50000, history_size=50, line_search_fn=\"strong_wolfe\")\n",
    "ITERATIONS = 20000\n",
    "\n",
    "# Load Data\n",
    "train_data = np.loadtxt(f'data/TecGrid-Cavity_1_0.01.dat', delimiter=' ', skiprows = 3)\n",
    "x = train_data[:,0:1]\n",
    "y = train_data[:,1:2]\n",
    "\n",
    "u = train_data[:,3:4]\n",
    "v = train_data[:,4:5]\n",
    "u_device = torch.from_numpy(u).float().to(device)\n",
    "v_device = torch.from_numpy(v).float().to(device)\n",
    "targets = torch.cat([u_device, v_device], axis = 1)\n",
    "\n",
    "\n",
    "for epoch in range(ITERATIONS):\n",
    "    adam_optimizer.zero_grad()\n",
    "\n",
    "    x_device = torch.from_numpy(x).float().to(device)\n",
    "    x_device.requires_grad = True\n",
    "    y_device = torch.from_numpy(y).float().to(device)\n",
    "    y_device.requires_grad = True\n",
    "\n",
    "    output = net(x_device, y_device)\n",
    "    # output = torch.cat([u_out, v_out], axis = 1)\n",
    "    loss = loss_fn(output, targets)\n",
    "    loss.backward()\n",
    "    adam_optimizer.step()\n",
    "\n",
    "    with torch.autograd.no_grad():\n",
    "    \tif (epoch+1) % 100 == 0:\n",
    "            print(epoch+1,\"Adam Loss:\",loss.data)\n",
    "\n",
    "last_loss = 0\n",
    "for i in range(50000):\n",
    "    lbfgs_optimizer.step(closure)\n",
    "    with torch.no_grad():\n",
    "        current_loss = closure()\n",
    "        print(i, \"L-BFGS-B Loss:\", current_loss.item())\n",
    "        if current_loss <= 1 * np.finfo(float).eps:\n",
    "            break\n",
    "        if current_loss == last_loss:\n",
    "            break\n",
    "        last_loss = current_loss\n",
    "print(\"Optimization finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01c5d2fa-ca6c-4679-8070-66d0d5d24208",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dif, result = net.predict(x_device, y_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e26d65e6-4e12-4afc-9a6e-e1b1fc3b0c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将Tensor转换为NumPy数组\n",
    "u_numpy = result[0].cpu().detach().numpy()\n",
    "v_numpy = result[1].cpu().detach().numpy()\n",
    "\n",
    "u_re = abs((u_numpy - u)/u)\n",
    "v_re = abs((v_numpy - v)/v)\n",
    "\n",
    "write_data = np.column_stack((x, y, u_numpy, v_numpy, u, v, u_re, v_re))\n",
    "\n",
    "np.savetxt('data/velocity_comparison.dat', write_data, delimiter=' ', \n",
    "           header='variables=\"x\",\"y\",\"u_predict\",\"v_predict\",\"u\",\"v\",\"u_re\",\"v_re\"', comments='', fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "987fee9f-b23d-4138-9086-096ed6fc4d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "ux = result_dif[0].cpu().detach().numpy()\n",
    "uy = result_dif[1].cpu().detach().numpy()\n",
    "vx = result_dif[2].cpu().detach().numpy()\n",
    "vy = result_dif[3].cpu().detach().numpy()\n",
    "\n",
    "gradient = np.column_stack((ux, uy, vx, vy))\n",
    "np.savetxt('data/square_cavity_flow.dat', write_data, delimiter=' ', comments='', fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9295202-5f27-408e-bf9d-91cdafd68a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_solution(X_star, u_star, index):\n",
    "    \n",
    "    lb = X_star.min(0)\n",
    "    ub = X_star.max(0)\n",
    "    nn = 200\n",
    "    x = np.linspace(lb[0], ub[0], nn)\n",
    "    y = np.linspace(lb[1], ub[1], nn)\n",
    "    X, Y = np.meshgrid(x,y)\n",
    "    \n",
    "    U_star = griddata(X_star, u_star.flatten(), (X, Y), method='cubic')\n",
    "    \n",
    "    plt.figure(index)\n",
    "    plt.pcolor(X,Y,U_star, cmap = 'jet')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    \n",
    "def axisEqual3D(ax):\n",
    "    extents = np.array([getattr(ax, 'get_{}lim'.format(dim))() for dim in 'xyz'])\n",
    "    sz = extents[:,1] - extents[:,0]\n",
    "    centers = np.mean(extents, axis=1)\n",
    "    maxsize = max(abs(sz))\n",
    "    r = maxsize/4\n",
    "    for ctr, dim in zip(centers, 'xyz'):\n",
    "        getattr(ax, 'set_{}lim'.format(dim))(ctr - r, ctr + r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df2e47f7-f87b-4500-accd-7cea95a8f7c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Session'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m t \u001b[38;5;241m=\u001b[39m TT\u001b[38;5;241m.\u001b[39mflatten()[:,\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;66;03m# NT x 1\u001b[39;00m\n\u001b[0;32m     50\u001b[0m rho \u001b[38;5;241m=\u001b[39m rho\u001b[38;5;241m.\u001b[39mflatten()[:,\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;66;03m# NT x 1\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m model \u001b[38;5;241m=\u001b[39m PhysicsInformedNN(y, t, rho, layers)\n\u001b[0;32m     53\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(iter_times)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Prediction\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 23\u001b[0m, in \u001b[0;36mPhysicsInformedNN.__init__\u001b[1;34m(self, y, t, rho, layers)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize_NN(layers)        \n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# tf placeholders and graph\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msess \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mSession(config\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mConfigProto(allow_soft_placement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     24\u001b[0m                                              log_device_placement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_tf \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mplaceholder(tf\u001b[38;5;241m.\u001b[39mfloat32, shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]])\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_tf \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mplaceholder(tf\u001b[38;5;241m.\u001b[39mfloat32, shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]])\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Session'"
     ]
    }
   ],
   "source": [
    "    N = y.shape[0]\n",
    "    T = t.shape[0]\n",
    "    \n",
    "    YY = np.tile(y[:,None], (1,T)) # N x T\n",
    "    TT = np.tile(t[:,None], (1,N)).T # N x T\n",
    "\n",
    "    y = YY.flatten()[:,None] # NT x 1\n",
    "    t = TT.flatten()[:,None] # NT x 1\n",
    "    rho = rho.flatten()[:,None] # NT x 1\n",
    "    \n",
    "    model = PhysicsInformedNN(y, t, rho, layers)\n",
    "    model.train(iter_times)\n",
    "\n",
    "    # Prediction\n",
    "    rho_pred, fdiff_pred = model.predict(y, t)\n",
    "\n",
    "    print(np.array(fdiff_pred).shape)\n",
    "    \n",
    "    scio.savemat(f'data/Diffusion_flow_new.mat', {'rho':rho, 'rho_t':fdiff_pred[0], 'rho_y':fdiff_pred[1], 'rho_yy':fdiff_pred[2], 'rho_3y':fdiff_pred[3]})\n",
    "\n",
    "    # Error\n",
    "    error_rho = np.linalg.norm(rho-rho_pred,2)/np.linalg.norm(rho,2)\n",
    "    a =  (rho-rho_pred)/rho\n",
    "    abs_error_rho = np.mean(abs(a))\n",
    " \n",
    "    print('Error rho: %e' % (error_rho)) \n",
    "    print('Abs error rho: %e' % (abs_error_rho))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6fcd5e-72f1-49d5-94bd-9edf2dfeb878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
